# LLM基礎知識 - ベースラインテスト

## 📋 テスト概要

**目的**: 現在のLLM関連知識レベルを具体的に測定し、学習計画を最適化する  
**所要時間**: 約20-30分  
**形式**: 選択肢問題 + 短答記述  
**評価基準**: 理解度レベル（完全理解/部分理解/未理解）を判定

---

## 🧠 Section 1: Transformer Architecture (10問)

### Q1. Transformerの基本構成要素
Transformerアーキテクチャの核となる仕組みは何ですか？

A) RNN (Recurrent Neural Network)  
B) CNN (Convolutional Neural Network)  
C) Attention Mechanism  
D) LSTM (Long Short-Term Memory)

**回答**: [　　]  
**確信度**: 高い / 中程度 / 低い  
**説明**: ________________________________

### Q2. Encoder-Decoderの役割
GPT-3のようなモデルは以下のどの構造ですか？

A) Encoder-only  
B) Decoder-only  
C) Encoder-Decoder  
D) Neither

**回答**: [　　]  
**確信度**: 高い / 中程度 / 低い  
**説明**: ________________________________

### Q3. 残差接続 (Residual Connection)
残差接続がTransformerで使われる主な理由は何ですか？

A) 計算速度の向上  
B) 勾配消失問題の軽減  
C) メモリ使用量の削減  
D) モデルサイズの縮小

**回答**: [　　]  
**確信度**: 高い / 中程度 / 低い  
**説明**: ________________________________

### Q4. Position Encoding
なぜTransformerにはposition encodingが必要ですか？

A) 計算を並列化するため  
B) モデルが単語の順序を理解するため  
C) メモリを節約するため  
D) 学習速度を上げるため

**回答**: [　　]  
**確信度**: 高い / 中程度 / 低い  
**説明**: ________________________________

### Q5. Layer Normalization vs Batch Normalization
TransformerでLayer Normalizationが使われる理由は？

**記述回答** (2-3行):
________________________________
________________________________

---

## 🎯 Section 2: Attention Mechanism (8問)

### Q6. Self-Attention の基本概念
Self-Attentionで「自分自身に注意を向ける」とは具体的に何を意味しますか？

**記述回答** (3-4行):
________________________________
________________________________
________________________________

### Q7. Query, Key, Value
Attention計算におけるQuery、Key、Valueの役割を説明してください。

**Query**: ________________________________
**Key**: ________________________________  
**Value**: ________________________________

### Q8. Multi-Head Attention
Multi-Head Attentionが単一のAttentionより優れている理由は？

A) 計算が速いから  
B) 異なる種類の関係性を並行して学習できるから  
C) メモリ使用量が少ないから  
D) 実装が簡単だから

**回答**: [　　]  
**確信度**: 高い / 中程度 / 低い  

### Q9. Attention Weight
Attention weightが示すものは何ですか？

**記述回答** (2-3行):
________________________________
________________________________

### Q10. Scaled Dot-Product Attention
なぜAttention計算で√dkで割るのですか？

A) 計算速度を上げるため  
B) Softmaxの入力値が大きくなりすぎるのを防ぐため  
C) メモリを節約するため  
D) 学習を安定化するため

**回答**: [　　]  
**確信度**: 高い / 中程度 / 低い  

---

## 📚 Section 3: Training Process (7問)

### Q11. Pre-training vs Fine-tuning
Pre-trainingとFine-tuningの違いを説明してください。

**Pre-training**: ________________________________
**Fine-tuning**: ________________________________
**主な違い**: ________________________________

### Q12. Language Modeling Objective
GPTのpre-trainingで使われる学習目標は？

A) 文の感情分析  
B) 次の単語の予測  
C) 文の翻訳  
D) 文の要約

**回答**: [　　]  

### Q13. BERT vs GPT
BERTとGPTの学習方式の主な違いは？

**記述回答** (3-4行):
________________________________
________________________________
________________________________

### Q14. RLHF (人間フィードバック強化学習)
RLHFが最近のLLMで重要視される理由は？

**記述回答** (2-3行):
________________________________
________________________________

### Q15. Transfer Learning
なぜLLMでTransfer Learningが効果的なのですか？

**記述回答** (2-3行):
________________________________
________________________________

---

## ⚙️ Section 4: 実践的知識 (8問)

### Q16. Model Selection
質問応答システムを作る場合、最適なモデルタイプは？

A) GPT系 (Decoder-only)  
B) BERT系 (Encoder-only)  
C) T5系 (Encoder-Decoder)  
D) どれでも同じ

**回答**: [　　]  
**理由**: ________________________________

### Q17. ハイパーパラメータ - Temperature
Temperatureパラメータが文章生成に与える影響は？

**高い値(1.0以上)**: ________________________________
**低い値(0.1-0.5)**: ________________________________

### Q18. Batch Size
Batch sizeを大きくした場合の影響は？

**メリット**: ________________________________
**デメリット**: ________________________________

### Q19. Learning Rate Scheduling
Learning rateを学習中に調整する理由は？

**記述回答** (2-3行):
________________________________
________________________________

### Q20. Tokenization
BPE (Byte Pair Encoding) の基本的な仕組みは？

**記述回答** (3-4行):
________________________________
________________________________
________________________________

### Q21. Context Length
「Context Length 4096」が意味することは？

A) モデルパラメータ数  
B) 一度に処理できる最大トークン数  
C) 学習データのサイズ  
D) レイヤー数

**回答**: [　　]  

### Q22. Quantization
モデルの8-bit量子化の主な目的は？

**記述回答** (2行):
________________________________
________________________________

---

## 🚀 Section 5: 最新動向・応用 (5問)

### Q23. GPT-4 vs Claude vs Gemini
これらのモデルの主な技術的差異を知っていますか？

**知っている内容** (自由記述):
________________________________
________________________________
________________________________

### Q24. In-Context Learning
In-Context Learningとは何ですか？

**記述回答** (2-3行):
________________________________
________________________________

### Q25. RAG (Retrieval Augmented Generation)
RAGシステムの基本的な仕組みは？

**記述回答** (3-4行):
________________________________
________________________________
________________________________

### Q26. Chain-of-Thought Prompting
Chain-of-Thought promptingの効果は？

**記述回答** (2-3行):
________________________________
________________________________

### Q27. 今後の学習優先度
以下の中で、最も学習したい・必要だと感じる領域は？（複数選択可）

□ Transformer内部構造の詳細理解  
□ ハイパーパラメータチューニング  
□ 最新モデル（GPT-4, Claude等）の技術詳細  
□ RAG実装とベクターデータベース  
□ プロンプトエンジニアリング  
□ モデル最適化（量子化、蒸留等）  
□ その他: ________________________________

---

## 📊 自己評価セクション

### 全体的な理解度
現在の自分のLLM知識レベルをどう評価しますか？

1. **理論的理解**: 1(全くわからない) ～ 10(完全に理解)  [　　]
2. **実装能力**: 1(全くできない) ～ 10(完全にできる)  [　　]  
3. **説明能力**: 1(説明できない) ～ 10(専門家レベル)  [　　]

### 学習の優先順位
どの分野を最優先で学習したいですか？

**1位**: ________________________________
**2位**: ________________________________  
**3位**: ________________________________

### 期待する学習成果
6週間後にどのレベルに到達したいですか？

**記述回答**:
________________________________
________________________________
________________________________

---

## 🎯 テスト完了後の処理

このテストを完了したら、以下の情報をお知らせください：

1. **回答状況**: 「全問回答完了」または「○問まで回答」
2. **所要時間**: 実際にかかった時間
3. **感想**: テストの難易度や気づいた点

この情報を基に、あなた専用の学習プランを最適化いたします！
